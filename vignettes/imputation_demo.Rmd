---
title: "Imputing missing data using rMIDAS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Imputing missing data using rMIDAS}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
LOCAL <- identical(Sys.getenv("LOCAL"), "true")
```

This vignette provides a brief demonstration of using **rMIDAS** to multiply impute missing data using the MIDAS imputation algorithm. We show how to use the package to multiply impute missing values using the Adult census data, a commonly used machine-learning dataset.

## Ensure your system is correctly configured

rMIDAS relies on Python to run the MIDAS imputation algorithm, and so you should ensure you have a Python 3.X environment installed on your machine. When the package is first loaded, it will try and automatically locate a suitable Python environment, but if this fails you will receive a warning message. When this occurs, users can manually specify a Python binary, virtualenv, or condaenv using the `set_python_` functions (see the help files for more information).

## Loading the data

Once **rMIDAS** is initialised, we can load in our data. For the purpose of this brief demonstration, we'll use a subset of the Adult census data (a commonly used machine learning dataset):

```{r, eval = LOCAL}
library(rMIDAS)

adult <- read.csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv",
                  row.names = 1)[1:1000,]
```

As the dataset has a very low proportion of missingness (one of the reasons it is favoured in machine learning communities), we randomly set approximately 10% of observed values as missing in each column using **rMIDAS'** inbuilt `add_missingness()` function:

```{r, eval = LOCAL}
set.seed(89)

adult <- add_missingness(adult, prop = 0.1)
```

Next, we list the categorical and binary variables in our data, and prepare the data for training using the `convert()` function. This function automatically preprocesses the data. Setting the `minmax_scale` parameter to `TRUE` ensures continuous variables are scaled to between 0 and 1, which can substantially improve convergence in the training step. All pre-processing steps can be reversed after we impute the missing values:

```{r, eval = LOCAL}

adult_cat <- c('workclass','marital_status','relationship','race','education','occupation','native_country')
adult_bin <- c('sex','class_labels')

# Apply rMIDAS preprocessing steps
adult_conv <- convert(adult, 
                      bin_cols = adult_bin, 
                      cat_cols = adult_cat,
                      minmax_scale = TRUE)
```

The data are now ready to be fed into the MIDAS algorithm, which involves a single call to the `train()` function. At this stage, we can specify the dimensions, input corruption proportion, and other hyperparameters of the MIDAS neural network as well as the number of epochs to train the network for:

```{r, eval = LOCAL}
# Train the model for 2 epochs
adult_train <- train(adult_conv,
                       training_epochs = 20,
                       layer_structure = c(128,128),
                       input_drop = 0.75,
                       seed = 89)
```

Once training is complete, we can generate any number of imputed datasets using the `complete()` function (here we generate 10 complete datasets). The completed data.frames can also be saved as '.csv' files by declaring `file` and `file_root` parameters (not demonstrated here). By default, this function will unscale continuous variables and convert binary and categorical variables back to their original shape. Since the MIDAS algorithm returns predicted probabilities, categorical and binary predictions can be generated using one of two options. When `fast = FALSE` (the default), the function uses the predicted probabilities for each level to take a weighted random draw from the set of category levels. If `fast = TRUE`, the function will select the category level with the highest predicted probability. If the imputed datasets are very large, or `complete()` is taking too long to run, users may wish to try setting `fast = TRUE`:

```{r, eval = LOCAL}

# Generate 10 imputed datasets
adult_complete <- complete(adult_train, m = 10)

# Inspect first imputed dataset:
head(adult_complete[[1]])
```

Finally, the `combine()` function allows users to run multiple imputation regression analysis according to Rubin's combination rules. This function wraps the `glm()` package, and as such can be passed `glm` options as additional arguments, and run different families of model (gaussian, binomial etc.):

```{r, eval = LOCAL}

# Estimate logit model using Rubin's Rules on 10 imputed datasets
adult_model <- combine("class_labels ~ hours_per_week + sex", 
                    adult_complete,
                    family = stats::binomial)

adult_model
```
